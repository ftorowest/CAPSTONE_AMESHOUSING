{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8f488d57",
   "metadata": {},
   "source": [
    "# CAPSTONE INVESTIGACIÓN OPERATIVA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "436e2c79",
   "metadata": {},
   "source": [
    "### Instalación de dependencias en el .venv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2368c965",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in c:\\users\\acer\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (25.2)\n",
      "Requirement already satisfied: numpy in c:\\users\\acer\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (2.2.4)\n",
      "Requirement already satisfied: pandas in c:\\users\\acer\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (2.3.2)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\acer\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (1.7.1)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\acer\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (3.10.1)\n",
      "Requirement already satisfied: joblib in c:\\users\\acer\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (1.5.2)\n",
      "Requirement already satisfied: gurobipy in c:\\users\\acer\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (12.0.3)\n",
      "Collecting gurobi-machinelearning\n",
      "  Using cached gurobi_machinelearning-1.5.5-py3-none-any.whl.metadata (6.8 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\acer\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\acer\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\acer\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: scipy>=1.8.0 in c:\\users\\acer\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from scikit-learn) (1.15.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\acer\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from scikit-learn) (3.6.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\acer\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from matplotlib) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\acer\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\acer\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from matplotlib) (4.57.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\acer\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from matplotlib) (1.4.8)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\acer\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from matplotlib) (24.2)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\acer\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from matplotlib) (11.1.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\acer\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from matplotlib) (3.2.3)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\acer\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Using cached gurobi_machinelearning-1.5.5-py3-none-any.whl (73 kB)\n",
      "Installing collected packages: gurobi-machinelearning\n",
      "Successfully installed gurobi-machinelearning-1.5.5\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install --upgrade pip\n",
    "!{sys.executable} -m pip install numpy pandas scikit-learn matplotlib joblib gurobipy gurobi-machinelearning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "409bbc11",
   "metadata": {},
   "source": [
    "### Importamos Librerias necesarias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b481cd19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============\n",
    "# Config & IO\n",
    "# ============\n",
    "import re, numpy as np, pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "from sklearn.model_selection import KFold, train_test_split\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "# === PASO 3.2 — Fallback SIN Pipeline (estimador puro) ===\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gurobipy as gp\n",
    "from gurobipy import GRB\n",
    "\n",
    "# Registramos convertidores sklearn por si acaso (aunque no usamos pipeline)\n",
    "import gurobi_ml, gurobi_ml.sklearn\n",
    "from gurobi_ml import add_predictor_constr\n",
    "\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "# ===============================\n",
    "# Comparación de modelos (CV 5-fold)\n",
    "# ===============================\n",
    "import time, numpy as np, pandas as pd\n",
    "from sklearn.model_selection import KFold, train_test_split\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.linear_model import LinearRegression, RidgeCV, LassoCV\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "429db8d8",
   "metadata": {},
   "source": [
    "### Prepocesamiento de la base de datos \"AmesHousing\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ff0301e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape: (2930, 82) | TARGET: SalePrice\n",
      "Numéricas: 37 | Categóricas: 38\n"
     ]
    }
   ],
   "source": [
    "# Ajusta la ruta a tu CSV Ames\n",
    "FILE = \"AmesHousing(in).csv\"   # cámbialo si está en otra carpeta\n",
    "\n",
    "def clean_col(c: str) -> str:\n",
    "    c = re.sub(r\"[^0-9A-Za-z_]+\", \"_\", c)\n",
    "    c = re.sub(r\"_+\", \"_\", c).strip(\"_\")\n",
    "    return c\n",
    "\n",
    "df = pd.read_csv(FILE)\n",
    "orig_cols = df.columns.tolist()\n",
    "df.columns = [clean_col(c) for c in df.columns]\n",
    "colmap = dict(zip(orig_cols, df.columns))\n",
    "\n",
    "#Arreglar variable Fence si es Na = 0, sino 1\n",
    "df[\"Fence\"] = df[\"Fence\"].apply(lambda x: 0 if pd.isna(x) else 1)\n",
    "\n",
    "\n",
    "# Detectar objetivo (SalePrice o similar)\n",
    "target_candidates = [c for c in df.columns if re.fullmatch(r\"(?i)sale_?price\", c)]\n",
    "assert target_candidates, \"No encontré la columna objetivo (SalePrice)\"\n",
    "TARGET = target_candidates[0]\n",
    "\n",
    "# Quitar columnas problemáticas/ID (ajusta si quieres)\n",
    "ID_COLS = [c for c in [\"Order\",\"PID\",\"Pool_QC\",\"Misc_Feature\",\"Alley\",\"Fireplace_Qu\"] if c in df.columns]\n",
    "\n",
    "X = df.drop(columns=[TARGET] + ID_COLS, errors=\"ignore\").copy()\n",
    "y = df[TARGET].copy()\n",
    "\n",
    "# Detectar tipos\n",
    "num_cols = [c for c in X.columns if np.issubdtype(X[c].dtype, np.number)]\n",
    "cat_cols = [c for c in X.columns if c not in num_cols]\n",
    "\n",
    "def make_ohe():\n",
    "    try:\n",
    "        return OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False)\n",
    "    except TypeError:\n",
    "        return OneHotEncoder(handle_unknown=\"ignore\", sparse=False)\n",
    "\n",
    "num_pre = Pipeline([\n",
    "    (\"imp\", SimpleImputer(strategy=\"median\")),\n",
    "    (\"scaler\", StandardScaler())\n",
    "])\n",
    "\n",
    "cat_pre = Pipeline([\n",
    "    (\"imp\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "    (\"ohe\", make_ohe())\n",
    "])\n",
    "\n",
    "pre = ColumnTransformer([\n",
    "    (\"num\", num_pre, num_cols),\n",
    "    (\"cat\", cat_pre, cat_cols)\n",
    "])\n",
    "\n",
    "print(f\"Shape: {df.shape} | TARGET: {TARGET}\")\n",
    "print(\"Numéricas:\", len(num_cols), \"| Categóricas:\", len(cat_cols))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eb2d53a",
   "metadata": {},
   "source": [
    "### Modelos  ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19115c9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>RMSE</th>\n",
       "      <th>MAE</th>\n",
       "      <th>R2</th>\n",
       "      <th>secs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>GBR</td>\n",
       "      <td>28747.239</td>\n",
       "      <td>19353.107</td>\n",
       "      <td>0.870</td>\n",
       "      <td>6.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>RF_400</td>\n",
       "      <td>29995.829</td>\n",
       "      <td>19433.458</td>\n",
       "      <td>0.859</td>\n",
       "      <td>4.94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>RF_200</td>\n",
       "      <td>30416.159</td>\n",
       "      <td>19582.617</td>\n",
       "      <td>0.855</td>\n",
       "      <td>2.47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Tree_d10</td>\n",
       "      <td>34650.419</td>\n",
       "      <td>22907.941</td>\n",
       "      <td>0.812</td>\n",
       "      <td>0.13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Tree_d14</td>\n",
       "      <td>36144.353</td>\n",
       "      <td>24278.598</td>\n",
       "      <td>0.795</td>\n",
       "      <td>0.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Linear</td>\n",
       "      <td>46632.372</td>\n",
       "      <td>21770.416</td>\n",
       "      <td>0.659</td>\n",
       "      <td>0.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>RidgeCV</td>\n",
       "      <td>46866.869</td>\n",
       "      <td>21757.656</td>\n",
       "      <td>0.656</td>\n",
       "      <td>0.04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>LassoCV</td>\n",
       "      <td>113788.555</td>\n",
       "      <td>30513.773</td>\n",
       "      <td>-1.030</td>\n",
       "      <td>0.30</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      model        RMSE        MAE     R2  secs\n",
       "4       GBR   28747.239  19353.107  0.870  6.80\n",
       "3    RF_400   29995.829  19433.458  0.859  4.94\n",
       "2    RF_200   30416.159  19582.617  0.855  2.47\n",
       "0  Tree_d10   34650.419  22907.941  0.812  0.13\n",
       "1  Tree_d14   36144.353  24278.598  0.795  0.10\n",
       "5    Linear   46632.372  21770.416  0.659  0.02\n",
       "6   RidgeCV   46866.869  21757.656  0.656  0.04\n",
       "7   LassoCV  113788.555  30513.773 -1.030  0.30"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Mejor por RMSE (CV): GBR\n",
      "Features usadas: ['Gr_Liv_Area', 'Total_Bsmt_SF', 'Garage_Area', 'Garage_Cars', 'Full_Bath', 'Fireplaces', 'Overall_Qual', 'Fence']\n",
      "\n",
      "Importancias del ganador:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Overall_Qual     0.590492\n",
       "Gr_Liv_Area      0.165508\n",
       "Total_Bsmt_SF    0.107026\n",
       "Garage_Cars      0.050787\n",
       "Garage_Area      0.045887\n",
       "Fireplaces       0.029425\n",
       "Full_Bath        0.010093\n",
       "Fence            0.000783\n",
       "dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "→ Recomendado para Gurobi (soporte y performance): GBR\n"
     ]
    }
   ],
   "source": [
    "\n",
    "CANDIDATES = [\n",
    "    \"Gr_Liv_Area\",\"Total_Bsmt_SF\",\"Garage_Area\",\n",
    "    \"Garage_Cars\",\"Full_Bath\",\"Fireplaces\",\"Overall_Qual\", \n",
    "]\n",
    "assert 'X' in globals() and 'y' in globals(), \"Debes tener X,y en memoria.\"\n",
    "X_fit = X[CANDIDATES].copy()\n",
    "X_fit = X_fit.fillna(X_fit.median(numeric_only=True))\n",
    "y_log = np.log1p(y)  # entrenamos en log1p\n",
    "\n",
    "# ---------- Modelos a probar ----------\n",
    "models = {\n",
    "    \"Tree_d10\": DecisionTreeRegressor(max_depth=10, min_samples_leaf=5, random_state=42),\n",
    "    \"Tree_d14\": DecisionTreeRegressor(max_depth=14, min_samples_leaf=3, random_state=42),\n",
    "\n",
    "    \"RF_200\": RandomForestRegressor(\n",
    "        n_estimators=200, max_depth=16, min_samples_leaf=3, max_features=\"sqrt\",\n",
    "        n_jobs=-1, random_state=42\n",
    "    ),\n",
    "    \"RF_400\": RandomForestRegressor(\n",
    "        n_estimators=400, max_depth=18, min_samples_leaf=2, max_features=\"sqrt\",\n",
    "        n_jobs=-1, random_state=42\n",
    "    ),\n",
    "\n",
    "    \"GBR\": GradientBoostingRegressor(\n",
    "        learning_rate=0.05, n_estimators=400, max_depth=3, subsample=0.9, random_state=42\n",
    "    ),\n",
    "\n",
    "    \"Linear\": LinearRegression(),\n",
    "    \"RidgeCV\": RidgeCV(alphas=np.logspace(-3,3,7)),\n",
    "    \"LassoCV\": LassoCV(cv=5, random_state=42, max_iter=5000)\n",
    "}\n",
    "\n",
    "def cv_eval_estimator(est, X, y_log, n_splits=5, seed=42):\n",
    "    \"\"\"Entrena en log1p(y) y reporta métricas en escala original.\"\"\"\n",
    "    kf = KFold(n_splits=n_splits, shuffle=True, random_state=seed)\n",
    "    y_true_all, y_pred_all = [], []\n",
    "    t0 = time.time()\n",
    "    for tr, te in kf.split(X):\n",
    "        Xt, Xv = X.iloc[tr], X.iloc[te]\n",
    "        if hasattr(y_log, \"iloc\"):\n",
    "            yt, yv = y_log.iloc[tr], y_log.iloc[te]\n",
    "        else:\n",
    "            yt, yv = y_log[tr], y_log[te]\n",
    "\n",
    "        m = est.__class__(**est.get_params())\n",
    "        m.fit(Xt, yt)\n",
    "        pred_v = np.expm1(m.predict(Xv))  # back to original scale\n",
    "        yv_o   = np.expm1(yv)\n",
    "        y_true_all.append(yv_o); y_pred_all.append(pred_v)\n",
    "    elapsed = time.time() - t0\n",
    "\n",
    "    y_true = np.concatenate(y_true_all); y_pred = np.concatenate(y_pred_all)\n",
    "    mse  = mean_squared_error(y_true, y_pred)   # <-- sin 'squared'\n",
    "    rmse = np.sqrt(mse)\n",
    "    mae  = mean_absolute_error(y_true, y_pred)\n",
    "    r2   = r2_score(y_true, y_pred)\n",
    "    return {\"RMSE\": rmse, \"MAE\": mae, \"R2\": r2, \"secs\": elapsed}\n",
    "\n",
    "# ---------- Evaluación ----------\n",
    "rows = []\n",
    "for name, est in models.items():\n",
    "    metrics = cv_eval_estimator(est, X_fit, y_log, n_splits=5, seed=42)\n",
    "    rows.append({\"model\": name, **{k: round(v,3) if k!='secs' else round(v,2) for k,v in metrics.items()}})\n",
    "results = pd.DataFrame(rows).sort_values(\"RMSE\")\n",
    "display(results)\n",
    "\n",
    "best_name = results.iloc[0][\"model\"]\n",
    "print(f\"\\nMejor por RMSE (CV): {best_name}\")\n",
    "\n",
    "# ---------- Entrenar todos en TODO el dataset para usarlos luego ----------\n",
    "fitted_models = {}\n",
    "for name, est in models.items():\n",
    "    m = est.__class__(**est.get_params())\n",
    "    m.fit(X_fit, y_log)\n",
    "    fitted_models[name] = m\n",
    "\n",
    "feat_names = list(X_fit.columns)\n",
    "print(\"Features usadas:\", feat_names)\n",
    "\n",
    "# ---------- Importancias / Coefs del ganador ----------\n",
    "winner = fitted_models[best_name]\n",
    "if hasattr(winner, \"feature_importances_\"):\n",
    "    imp = pd.Series(winner.feature_importances_, index=feat_names).sort_values(ascending=False)\n",
    "    print(\"\\nImportancias del ganador:\"); display(imp)\n",
    "elif hasattr(winner, \"coef_\"):\n",
    "    coefs = pd.Series(np.ravel(winner.coef_), index=feat_names).sort_values(key=np.abs, ascending=False)\n",
    "    print(\"\\nCoeficientes (ordenados por |coef|) del ganador:\"); display(coefs)\n",
    "else:\n",
    "    print(\"\\nEl modelo ganador no expone importancias/coefs directamente.\")\n",
    "\n",
    "# ---------- Recomendado para Gurobi ----------\n",
    "embeedable = {\"Tree_d10\",\"Tree_d14\",\"RF_200\",\"RF_400\",\"GBR\",\"Linear\",\"RidgeCV\"}\n",
    "df_emb = results[results[\"model\"].isin(embeedable)].sort_values(\"RMSE\")\n",
    "best_emb_name = df_emb.iloc[0][\"model\"]\n",
    "best_emb_model = fitted_models[best_emb_name]\n",
    "print(f\"\\n→ Recomendado para Gurobi (soporte y performance): {best_emb_name}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae7135ba",
   "metadata": {},
   "source": [
    "### Parámetros del modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75741b5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Caso base:\n",
      "Gr_Liv_Area      1494.0\n",
      "Total_Bsmt_SF    1494.0\n",
      "Garage_Area       840.0\n",
      "Garage_Cars         3.0\n",
      "Full_Bath           2.0\n",
      "Fireplaces          1.0\n",
      "Overall_Qual        7.0\n",
      "Fence               0.0\n",
      "Name: 2100, dtype: float64\n",
      "Precio base predicho: 232,757\n",
      "Precio real: 279,500\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------\n",
    "# 2) Definir baseline, límites y costos\n",
    "# ----------------------------\n",
    "ID_CASA = 2100\n",
    "\n",
    "# baseline con las MISMAS columnas usadas para entrenar el predictor\n",
    "baseline = X_fit.iloc[ID_CASA].astype(float)\n",
    "\n",
    "print(\"Caso base:\")\n",
    "print(baseline)\n",
    "predictor = best_emb_model\n",
    "# predicción del modelo (viene en log1p -> deshacer con expm1)\n",
    "pred_log = predictor.predict(baseline.to_frame().T)[0]\n",
    "precio_base_pred = float(np.expm1(pred_log))\n",
    "print(f\"Precio base predicho: {precio_base_pred:,.0f}\")\n",
    "\n",
    "# precio real en escala original (NO aplicar expm1)\n",
    "precio_real = float(y.iloc[ID_CASA])  # ojo: iloc\n",
    "print(f\"Precio real: {precio_real:,.0f}\")\n",
    "\n",
    "# percentil 95 para cada feature (usado luego para cotas superiores)\n",
    "quant95 = X_fit.quantile(0.95)\n",
    "\n",
    "\n",
    "default_costs = {\n",
    "    \"Gr_Liv_Area\":   200,     # USD por ft²\n",
    "    \"Total_Bsmt_SF\":  80,     # USD por ft²\n",
    "    \"Garage_Area\":    60,     # USD por ft²\n",
    "    \"Garage_Cars\":  17000,    # USD por puesto\n",
    "    \"Full_Bath\":    25000,    # USD por baño\n",
    "    \"Fireplaces\":    6000,    # USD por chimenea\n",
    "    \"Overall_Qual\": 20000    # USD por subir 1 punto (proxy)\n",
    "}\n",
    "\n",
    "room = {\n",
    "    \"Gr_Liv_Area\":   400.0,\n",
    "    \"Total_Bsmt_SF\": 300.0,\n",
    "    \"Garage_Area\":   250.0,\n",
    "    \"Garage_Cars\":     1.0,   # si tu lote permite 2, súbelo a 2.0\n",
    "    \"Full_Bath\":       1.0,\n",
    "    \"Fireplaces\":      1.0,\n",
    "    \"Overall_Qual\":    1.0    # 1 punto suele ser más realista que 2\n",
    "}\n",
    "\n",
    "bounds, costs = {}, {}\n",
    "for c in feat_names:\n",
    "    base = float(baseline.get(c, X_fit[c].median()))\n",
    "    ub_by_room = base + room.get(c, 0.0)\n",
    "    ub_by_p95  = float(quant95[c])\n",
    "    lb = base\n",
    "    ub = max(lb, min(ub_by_room, ub_by_p95))\n",
    "    bounds[c] = (lb, ub)\n",
    "    costs[c]  = default_costs.get(c, 0.0)\n",
    "\n",
    "BUDGET = 200_000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e31b30cb",
   "metadata": {},
   "source": [
    "### Guroby"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "87ed2c5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "→ Usando modelo: GBR\n",
      "Set parameter Username\n",
      "Set parameter LicenseID to value 2670211\n",
      "Academic license - for non-commercial use only - expires 2026-05-23\n",
      "Set parameter OutputFlag to value 1\n",
      "Gurobi Optimizer version 12.0.3 build v12.0.3rc0 (win64 - Windows 10.0 (19045.2))\n",
      "\n",
      "CPU model: 11th Gen Intel(R) Core(TM) i3-1115G4 @ 3.00GHz, instruction set [SSE2|AVX|AVX2|AVX512]\n",
      "Thread count: 2 physical cores, 4 logical processors, using up to 4 threads\n",
      "\n",
      "Optimize a model with 1202 rows, 3518 columns and 4316 nonzeros\n",
      "Model fingerprint: 0x2a3e757b\n",
      "Model has 779 simple general constraints\n",
      "  778 INDICATOR, 1 PWL\n",
      "Variable types: 406 continuous, 3112 integer (3108 binary)\n",
      "Coefficient statistics:\n",
      "  Matrix range     [5e-02, 3e+04]\n",
      "  Objective range  [1e+00, 3e+04]\n",
      "  Bounds range     [1e+00, 2e+03]\n",
      "  RHS range        [9e-03, 9e+05]\n",
      "  PWLCon x range   [1e+01, 1e+01]\n",
      "  PWLCon y range   [7e+04, 4e+05]\n",
      "  GenCon rhs range [1e-05, 2e+03]\n",
      "  GenCon coe range [1e+00, 1e+00]\n",
      "Presolve removed 897 rows and 3321 columns\n",
      "Presolve time: 0.17s\n",
      "Presolved: 305 rows, 197 columns, 998 nonzeros\n",
      "Variable types: 29 continuous, 168 integer (166 binary)\n",
      "Found heuristic solution: objective 184114.24799\n",
      "Found heuristic solution: objective 199869.01330\n",
      "\n",
      "Root relaxation: objective 2.669609e+05, 233 iterations, 0.01 seconds (0.00 work units)\n",
      "\n",
      "    Nodes    |    Current Node    |     Objective Bounds      |     Work\n",
      " Expl Unexpl |  Obj  Depth IntInf | Incumbent    BestBd   Gap | It/Node Time\n",
      "\n",
      "     0     0 266960.884    0   36 199869.013 266960.884  33.6%     -    0s\n",
      "     0     0 264716.409    0   35 199869.013 264716.409  32.4%     -    0s\n",
      "H    0     0                    234522.65830 264716.409  12.9%     -    0s\n",
      "     0     0 259491.744    0   40 234522.658 259491.744  10.6%     -    0s\n",
      "H    0     0                    244393.98968 259436.645  6.16%     -    0s\n",
      "H    0     0                    245371.24011 259436.645  5.73%     -    0s\n",
      "     0     0 258893.674    0   46 245371.240 258893.674  5.51%     -    0s\n",
      "     0     0 258684.932    0   50 245371.240 258684.932  5.43%     -    0s\n",
      "     0     0 257946.284    0    6 245371.240 257946.284  5.12%     -    0s\n",
      "     0     0 257777.603    0    6 245371.240 257777.603  5.06%     -    0s\n",
      "     0     0 256200.700    0   98 245371.240 256200.700  4.41%     -    0s\n",
      "H    0     0                    251664.61363 256134.808  1.78%     -    0s\n",
      "     0     0 255827.106    0   57 251664.614 255827.106  1.65%     -    0s\n",
      "     0     0 255825.945    0   63 251664.614 255825.945  1.65%     -    0s\n",
      "     0     0 254512.094    0   64 251664.614 254512.094  1.13%     -    0s\n",
      "     0     0 253916.662    0   22 251664.614 253916.662  0.89%     -    0s\n",
      "     0     0          -    0      251664.614 251681.061  0.01%     -    0s\n",
      "\n",
      "Explored 1 nodes (562 simplex iterations) in 0.59 seconds (0.07 work units)\n",
      "Thread count was 4 (of 4 available processors)\n",
      "\n",
      "Solution count 6: 251665 245371 244394 ... 184114\n",
      "\n",
      "Optimal solution found (tolerance 1.00e-04)\n",
      "Best objective 2.516646136272e+05, best bound 2.516810606728e+05, gap 0.0065%\n",
      "\n",
      "=== SOLUCIÓN ===\n",
      "Modelo usado    : GBR\n",
      "Objetivo        : profit\n",
      "Precio antes    : 232,757\n",
      "Precio después  : 295,865\n",
      "Gasto estimado  : 44,200  (<= presupuesto 200,000)\n",
      "Ganancia        : 18,908   |  ROI: 0.43\n",
      "\n",
      "Valores finales de features:\n",
      " - Gr_Liv_Area    :   1494.000  (base   1494.000, Δ=    +0.000)\n",
      " - Total_Bsmt_SF  :   1721.500  (base   1494.000, Δ=  +227.500)\n",
      " - Garage_Area    :    840.000  (base    840.000, Δ=    +0.000)\n",
      " - Garage_Cars    :      3.000  (base      3.000, Δ=    +0.000)\n",
      " - Full_Bath      :      2.000  (base      2.000, Δ=    +0.000)\n",
      " - Fireplaces     :      2.000  (base      1.000, Δ=    +1.000)\n",
      " - Overall_Qual   :      8.000  (base      7.000, Δ=    +1.000)\n",
      " - Fence          :      0.000  (base      0.000, Δ=    +0.000)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ---------- Parámetros ----------\n",
    "MODEL_NAME     = \"auto\"          # \"auto\" o uno de: \"RF_400\",\"GBR\",\"Tree_d14\",\"Linear\",\"RidgeCV\", etc.\n",
    "OBJECTIVE_MODE = \"profit\"        # \"price\" | \"profit\" | \"roi\"\n",
    "ROI_MIN        = 0.10            # usado sólo si OBJECTIVE_MODE == \"roi\"\n",
    "PWL_K          = 25              # segmentos para PWL (log1p -> price)\n",
    "\n",
    "# ---------- Resolver el estimador ----------\n",
    "assert 'fitted_models' in globals(), \"Falta 'fitted_models' (corre la celda de comparación).\"\n",
    "compatibles = {\"Tree_d10\",\"Tree_d14\",\"RF_200\",\"RF_400\",\"GBR\",\"Linear\",\"RidgeCV\"}\n",
    "\n",
    "if MODEL_NAME == \"auto\":\n",
    "    if 'best_emb_name' in globals():\n",
    "        chosen = best_emb_name\n",
    "    elif 'best_name' in globals():\n",
    "        if best_name in compatibles:\n",
    "            chosen = best_name\n",
    "        else:\n",
    "            assert 'results' in globals(), \"Falta 'results' para elegir el mejor compatible.\"\n",
    "            chosen = results[results[\"model\"].isin(compatibles)].sort_values(\"RMSE\").iloc[0][\"model\"]\n",
    "    else:\n",
    "        chosen = next((k for k in fitted_models if k in compatibles), None)\n",
    "        assert chosen is not None, \"No hay modelo compatible en 'fitted_models'.\"\n",
    "else:\n",
    "    chosen = MODEL_NAME\n",
    "    assert chosen in fitted_models, f\"'{chosen}' no está en fitted_models.\"\n",
    "\n",
    "predictor = fitted_models[chosen]\n",
    "print(f\"→ Usando modelo: {chosen}\")\n",
    "\n",
    "# ---------- Columnas esperadas ----------\n",
    "if hasattr(predictor, \"feature_names_in_\"):\n",
    "    feat_names_model = list(predictor.feature_names_in_)\n",
    "else:\n",
    "    assert 'feat_names' in globals(), \"Falta 'feat_names'.\"\n",
    "    feat_names_model = list(feat_names)\n",
    "\n",
    "for need in (\"baseline\",\"bounds\",\"costs\",\"BUDGET\"):\n",
    "    assert need in globals(), f\"Falta '{need}' en el entorno.\"\n",
    "missing = [c for c in feat_names_model if c not in bounds or c not in baseline.index or c not in costs]\n",
    "assert not missing, f\"Faltan claves en bounds/baseline/costs para: {missing}\"\n",
    "\n",
    "# ---------- Modelo de optimización ----------\n",
    "m = gp.Model(f\"ames_renov_opt_{chosen}\")\n",
    "\n",
    "int_like = {\"Garage_Cars\",\"Full_Bath\",\"Fireplaces\",\"Overall_Qual\"}\n",
    "x = {}\n",
    "for c in feat_names_model:\n",
    "    lb, ub = bounds[c]\n",
    "    if c in int_like:\n",
    "        lb_i, ub_i = int(np.floor(lb)), int(np.ceil(ub))\n",
    "        if ub_i < lb_i: ub_i = lb_i\n",
    "        x[c] = m.addVar(lb=lb_i, ub=ub_i, vtype=GRB.INTEGER,   name=c)\n",
    "    else:\n",
    "        x[c] = m.addVar(lb=float(lb), ub=float(ub), vtype=GRB.CONTINUOUS, name=c)\n",
    "\n",
    "# Presupuesto (sólo mejoras)\n",
    "cost_expr = gp.quicksum(costs[c] * (x[c] - float(baseline[c])) for c in feat_names_model)\n",
    "m.addConstr(cost_expr <= float(BUDGET), name=\"Budget\")\n",
    "\n",
    "# (Opcional) ejemplo garaje acoplado:\n",
    "# STALL_SF = 200.0\n",
    "# if \"Garage_Area\" in x and \"Garage_Cars\" in x:\n",
    "#     m.addConstr((x[\"Garage_Area\"] - float(baseline[\"Garage_Area\"])) >=\n",
    "#                 STALL_SF * (x[\"Garage_Cars\"] - float(baseline[\"Garage_Cars\"])),\n",
    "#                 name=\"GarageAreaCoversStalls\")\n",
    "\n",
    "# DF 1xN con el orden exacto de columnas\n",
    "x_df = pd.DataFrame([[x[c] for c in feat_names_model]], columns=feat_names_model)\n",
    "\n",
    "# Variable de salida del predictor (log1p del precio)\n",
    "y_pred_log = m.addVar(name=\"y_pred_log\")\n",
    "\n",
    "add_predictor_constr(\n",
    "    gp_model=m,\n",
    "    predictor=predictor,\n",
    "    input_vars=x_df,\n",
    "    output_vars=y_pred_log\n",
    ")\n",
    "\n",
    "# ---------- Objetivo ----------\n",
    "if OBJECTIVE_MODE == \"price\":\n",
    "    # Maximiza log1p(price): suficiente porque exp() es monótona\n",
    "    m.setObjective(y_pred_log, GRB.MAXIMIZE)\n",
    "\n",
    "else:\n",
    "    # Necesitamos precio en escala original: price ≈ exp(y_log) - 1 vía PWL\n",
    "    # Rango PWL desde tus datos (más robusto que fijo)\n",
    "    y_log_all = np.log1p(y) if hasattr(y, \"__len__\") else pd.Series([0.0])\n",
    "    ymin = float(np.percentile(y_log_all, 1))\n",
    "    ymax = float(np.percentile(y_log_all, 99))\n",
    "    if not np.isfinite(ymin) or not np.isfinite(ymax) or ymin >= ymax:\n",
    "        ymin, ymax = 10.5, 13.5  # fallback razonable\n",
    "\n",
    "    xs = np.linspace(ymin, ymax, int(PWL_K)).tolist()\n",
    "    ys = (np.expm1(xs)).tolist()\n",
    "\n",
    "    price = m.addVar(name=\"price\")                      # USD (o tu moneda de y)\n",
    "    m.addGenConstrPWL(y_pred_log, price, xs, ys, name=\"log_to_price\")\n",
    "\n",
    "    # Precio baseline (constante, mismo predictor)\n",
    "    baseline_vec = pd.DataFrame([baseline[feat_names_model].to_dict()], columns=feat_names_model)\n",
    "    price_before = float(np.expm1(predictor.predict(baseline_vec))[0])\n",
    "\n",
    "    if OBJECTIVE_MODE == \"profit\":\n",
    "        # Max (precio_after - costo)\n",
    "        m.setObjective(price - cost_expr, GRB.MAXIMIZE)\n",
    "    elif OBJECTIVE_MODE == \"roi\":\n",
    "        # Max (precio_after - costo) con ROI mínimo\n",
    "        m.addConstr(price - price_before >= ROI_MIN * cost_expr, name=\"ROImin\")\n",
    "        m.setObjective(price - cost_expr, GRB.MAXIMIZE)\n",
    "    else:\n",
    "        raise ValueError(\"OBJECTIVE_MODE debe ser 'price', 'profit' o 'roi'.\")\n",
    "\n",
    "# ---------- Resolver ----------\n",
    "m.Params.OutputFlag = 1\n",
    "m.optimize()\n",
    "\n",
    "# ---------- Reporte ----------\n",
    "if m.SolCount > 0:\n",
    "    opt_vec = pd.DataFrame([{c: x[c].X for c in feat_names_model}], columns=feat_names_model)\n",
    "\n",
    "    price_before_rep = float(np.expm1(predictor.predict(baseline_vec))[0])\n",
    "    if OBJECTIVE_MODE == \"price\":\n",
    "        price_after_rep = float(np.expm1(y_pred_log.X))  # aprox (sin PWL)\n",
    "    else:\n",
    "        price_after_rep = float(price.X)                 # con PWL\n",
    "\n",
    "    deltas = {c: x[c].X - float(baseline[c]) for c in feat_names_model}\n",
    "    spent  = float(cost_expr.getValue())\n",
    "    profit = price_after_rep - price_before_rep - spent\n",
    "    roi    = (profit / spent) if spent > 0 else float('nan')\n",
    "\n",
    "    print(\"\\n=== SOLUCIÓN ===\")\n",
    "    print(f\"Modelo usado    : {chosen}\")\n",
    "    print(f\"Objetivo        : {OBJECTIVE_MODE}\")\n",
    "    print(f\"Precio antes    : {price_before_rep:,.0f}\")\n",
    "    print(f\"Precio después  : {price_after_rep:,.0f}\")\n",
    "    print(f\"Gasto estimado  : {spent:,.0f}  (<= presupuesto {BUDGET:,.0f})\")\n",
    "    print(f\"Ganancia        : {profit:,.0f}   |  ROI: {roi:,.2f}\")\n",
    "\n",
    "    print(\"\\nValores finales de features:\")\n",
    "    for c in feat_names_model:\n",
    "        print(f\" - {c:15s}: {x[c].X:10.3f}  (base {float(baseline[c]):10.3f}, Δ={deltas[c]:+10.3f})\")\n",
    "else:\n",
    "    print(\"No se encontró solución factible.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
